{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ç—…åºŠå§¿å‹¢ç›£æ¸¬ - èªç¾©å¼•å°ç†±æˆåƒè¶…è§£æåº¦è¨“ç·´ï¼ˆå®Œæ•´ç‰ˆï¼‰\n",
    "\n",
    "**å°ˆæ¡ˆ**: ç†±æˆåƒè¶…è§£æåº¦ + å§¿å‹¢è¾¨è­˜  \n",
    "**ä½œè€…**: rochi190  \n",
    "**æ—¥æœŸ**: 2025-11-03  \n",
    "\n",
    "## è¨“ç·´ç­–ç•¥\n",
    "```\n",
    "éšæ®µ 1: RGB äººé«”åˆ†å‰² (UNet)\n",
    "  RGB (é«˜è§£æåº¦) â†’ Segmentation UNet â†’ Person Mask\n",
    "\n",
    "éšæ®µ 2: èªç¾©å¼•å°è¶…è§£æåº¦ (Guided SR)\n",
    "  è¼¸å…¥: Thermal LR (32Ã—24) + Person Mask (é«˜è§£æåº¦)\n",
    "  è¼¸å‡º: Thermal HR (256Ã—256)\n",
    "  Loss: L1 Loss + Perceptual Loss + Person Region Focus Loss\n",
    "\n",
    "éšæ®µ 3: å§¿å‹¢è¾¨è­˜ (Pose Classifier)\n",
    "  Thermal HR â†’ CNN â†’ Pose Class (lying/sitting/fallen)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 0: ç’°å¢ƒè¨­å®š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Matplotlib\n",
    "plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei', 'DejaVu Sans']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# éš¨æ©Ÿç¨®å­\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ç—…åºŠå§¿å‹¢ç›£æ¸¬ç³»çµ± - è¨“ç·´æ¨¡çµ„\")\n",
    "print(\"=\"*70)\n",
    "print(f\"åŸ·è¡Œæ™‚é–“: {pd.Timestamp.now()}\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"è¨“ç·´è£ç½®: {device}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: è·¯å¾‘èˆ‡åƒæ•¸é…ç½®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è·¯å¾‘\n",
    "BASE_DIR = Path(os.getcwd())\n",
    "OUTPUT_DIR = BASE_DIR / 'output'\n",
    "\n",
    "TRAINING_DIR = OUTPUT_DIR / 'training_dataset'\n",
    "ADAPTIVE_DIR = OUTPUT_DIR / 'adaptive_pairing'\n",
    "MODELS_DIR = OUTPUT_DIR / 'trained_models'\n",
    "LOGS_DIR = OUTPUT_DIR / 'training_logs'\n",
    "RESULTS_DIR = OUTPUT_DIR / 'training_results'\n",
    "\n",
    "for d in [MODELS_DIR, LOGS_DIR, RESULTS_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# è³‡æ–™\n",
    "LABELS_CSV = TRAINING_DIR / 'pose_labels_with_thermal.csv'\n",
    "MASKS_DIR = TRAINING_DIR / 'person_masks'\n",
    "PAIRS_JSON = ADAPTIVE_DIR / 'pairs_mapping.json'\n",
    "\n",
    "# è¨“ç·´åƒæ•¸\n",
    "STAGE1_CONFIG = {\n",
    "    'name': 'RGB_Segmentation',\n",
    "    'epochs': 50,\n",
    "    'batch_size': 16,\n",
    "    'learning_rate': 1e-4,\n",
    "    'input_size': (256, 256),\n",
    "}\n",
    "\n",
    "STAGE2_CONFIG = {\n",
    "    'name': 'Semantic_Guided_SR',\n",
    "    'epochs': 100,\n",
    "    'batch_size': 32,\n",
    "    'learning_rate': 1e-4,\n",
    "    'thermal_input_size': (24, 32),\n",
    "    'thermal_output_size': (256, 256),\n",
    "    'scale_factor': 8,\n",
    "    'lambda_l1': 1.0,\n",
    "    'lambda_perceptual': 0.1,\n",
    "    'lambda_person_focus': 0.5,\n",
    "}\n",
    "\n",
    "# éšæ®µ 3: å§¿å‹¢è¾¨è­˜é…ç½®\n",
    "STAGE3_CONFIG = {\n",
    "    'name': 'Pose_Classifier',\n",
    "    'epochs': 50,\n",
    "    'batch_size': 32,\n",
    "    'learning_rate': 1e-3,\n",
    "    'num_classes': 3,  # ğŸ”§ æ”¹ç‚º 3 é¡ï¼ˆlying, sitting, fallenï¼‰\n",
    "}\n",
    "\n",
    "TRAIN_SPLIT = 0.8\n",
    "VAL_SPLIT = 0.1\n",
    "TEST_SPLIT = 0.1\n",
    "\n",
    "print(f\"è·¯å¾‘æª¢æŸ¥:\")\n",
    "print(f\"  Labels: {'âœ“' if LABELS_CSV.exists() else 'âœ—'} {LABELS_CSV}\")\n",
    "print(f\"  Masks: {'âœ“' if MASKS_DIR.exists() else 'âœ—'} {MASKS_DIR}\")\n",
    "print(f\"  Pairs: {'âœ“' if PAIRS_JSON.exists() else 'âœ—'} {PAIRS_JSON}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: æº–å‚™è¨“ç·´è³‡æ–™ï¼ˆå¾ Adaptive Pairing æå–ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¾ adaptive pairing è¼‰å…¥é…å°\n",
    "with open(PAIRS_JSON, 'r') as f:\n",
    "    pairs_data = json.load(f)\n",
    "\n",
    "print(f\"è¼‰å…¥ {len(pairs_data)} å€‹é…å°\")\n",
    "\n",
    "# éœ€è¦å…ˆæå– Thermal å’Œ RGB åœ–ç‰‡ä¸¦å„²å­˜ç‚º .npy\n",
    "# (é€™éƒ¨åˆ†åœ¨ 01 ä¸­å·²ç¶“å®Œæˆï¼Œé€™è£¡å‡è¨­è³‡æ–™å·²å­˜åœ¨)\n",
    "\n",
    "# æª¢æŸ¥æ˜¯å¦å·²æœ‰ aligned_dataset\n",
    "ALIGNED_DIR = OUTPUT_DIR / 'aligned_dataset'\n",
    "THERMAL_DIR = ALIGNED_DIR / 'thermal'\n",
    "RGB_DIR = ALIGNED_DIR / 'rgb'\n",
    "\n",
    "if not THERMAL_DIR.exists() or not RGB_DIR.exists():\n",
    "    print(\"\\nâš ï¸ è­¦å‘Š: aligned_dataset ä¸å­˜åœ¨\")\n",
    "    print(\"è«‹å…ˆåŸ·è¡Œä»¥ä¸‹æ­¥é©Ÿæå–è³‡æ–™:\")\n",
    "    print(\"1. å¾ thermal log é‡æ–°è¼‰å…¥åœ–ç‰‡\")\n",
    "    print(\"2. å¾ AVI æå–å°æ‡‰çš„ RGB å¹€\")\n",
    "    print(\"3. å„²å­˜ç‚º .npy æª”æ¡ˆ\")\n",
    "    raise FileNotFoundError(\"è«‹å…ˆæº–å‚™ aligned_dataset\")\n",
    "\n",
    "print(f\"\\nAligned dataset:\")\n",
    "print(f\"  Thermal: {THERMAL_DIR}\")\n",
    "print(f\"  RGB: {RGB_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: è³‡æ–™é›†é¡åˆ¥å®šç¾©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RGBSegmentationDataset(Dataset):\n",
    "    def __init__(self, labels_df, rgb_dir, masks_dir, input_size=(256, 256)):\n",
    "        self.labels_df = labels_df\n",
    "        self.rgb_dir = rgb_dir\n",
    "        self.masks_dir = masks_dir\n",
    "        self.input_size = input_size\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels_df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.labels_df.iloc[idx]\n",
    "        \n",
    "        # RGB\n",
    "        rgb_path = self.rgb_dir / f\"{row['pair_id']}_rgb.npy\"\n",
    "        rgb = np.load(rgb_path)\n",
    "        rgb = cv2.resize(rgb, self.input_size)\n",
    "        rgb = rgb.astype(np.float32) / 255.0\n",
    "        \n",
    "        # Mask\n",
    "        mask_path = self.masks_dir / row['mask_file']\n",
    "        mask = cv2.imread(str(mask_path), cv2.IMREAD_GRAYSCALE)\n",
    "        mask = cv2.resize(mask, self.input_size, interpolation=cv2.INTER_NEAREST)\n",
    "        mask = (mask > 127).astype(np.float32)\n",
    "        \n",
    "        rgb = torch.from_numpy(rgb).permute(2, 0, 1)\n",
    "        mask = torch.from_numpy(mask).unsqueeze(0)\n",
    "        \n",
    "        return rgb, mask\n",
    "\n",
    "\n",
    "class SemanticGuidedSRDataset(Dataset):\n",
    "    def __init__(self, labels_df, thermal_dir, rgb_dir, masks_dir, \n",
    "                 thermal_input_size=(24, 32), output_size=(256, 256)):\n",
    "        self.labels_df = labels_df\n",
    "        self.thermal_dir = thermal_dir\n",
    "        self.rgb_dir = rgb_dir\n",
    "        self.masks_dir = masks_dir\n",
    "        self.thermal_input_size = thermal_input_size\n",
    "        self.output_size = output_size\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels_df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.labels_df.iloc[idx]\n",
    "        \n",
    "        # Thermal LR\n",
    "        thermal_path = self.thermal_dir / f\"{row['pair_id']}_thermal.npy\"\n",
    "        thermal_lr = np.load(thermal_path)\n",
    "        thermal_lr = cv2.resize(thermal_lr, (self.thermal_input_size[1], self.thermal_input_size[0]))\n",
    "        thermal_lr = thermal_lr.astype(np.float32) / 255.0\n",
    "        \n",
    "        # RGB (GT)\n",
    "        rgb_path = self.rgb_dir / f\"{row['pair_id']}_rgb.npy\"\n",
    "        rgb = np.load(rgb_path)\n",
    "        rgb_gray = cv2.cvtColor(rgb, cv2.COLOR_RGB2GRAY)\n",
    "        rgb_gray = cv2.resize(rgb_gray, self.output_size)\n",
    "        rgb_gray = rgb_gray.astype(np.float32) / 255.0\n",
    "        \n",
    "        # Person Mask\n",
    "        mask_path = self.masks_dir / row['mask_file']\n",
    "        person_mask = cv2.imread(str(mask_path), cv2.IMREAD_GRAYSCALE)\n",
    "        person_mask = cv2.resize(person_mask, self.output_size, interpolation=cv2.INTER_NEAREST)\n",
    "        person_mask = (person_mask > 127).astype(np.float32)\n",
    "        \n",
    "        thermal_lr = torch.from_numpy(thermal_lr).unsqueeze(0)\n",
    "        rgb_gray = torch.from_numpy(rgb_gray).unsqueeze(0)\n",
    "        person_mask = torch.from_numpy(person_mask).unsqueeze(0)\n",
    "        \n",
    "        return {\n",
    "            'thermal_lr': thermal_lr,\n",
    "            'person_mask': person_mask,\n",
    "            'rgb_gray_hr': rgb_gray,\n",
    "            'pair_id': row['pair_id']\n",
    "        }\n",
    "\n",
    "\n",
    "class PoseClassificationDataset(Dataset):\n",
    "    def __init__(self, labels_df, thermal_hr_dir, input_size=(256, 256)):\n",
    "        # ğŸ”§ åªä¿ç•™ 3 å€‹æœ‰æ•ˆå§¿å‹¢\n",
    "        valid_poses = ['lying', 'sitting', 'fallen']\n",
    "        self.labels_df = labels_df[labels_df['pose_label'].isin(valid_poses)].reset_index(drop=True)\n",
    "        \n",
    "        self.thermal_hr_dir = thermal_hr_dir\n",
    "        self.input_size = input_size\n",
    "        \n",
    "        # ğŸ”§ é¡åˆ¥æ˜ å°„ï¼ˆ3 é¡ï¼‰\n",
    "        self.class_to_idx = {\n",
    "            'lying': 0, \n",
    "            'sitting': 1, \n",
    "            'fallen': 2\n",
    "        }\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels_df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.labels_df.iloc[idx]\n",
    "        \n",
    "        thermal_hr_path = self.thermal_hr_dir / f\"{row['pair_id']}_thermal_hr.npy\"\n",
    "        thermal_hr = np.load(thermal_hr_path)\n",
    "        thermal_hr = cv2.resize(thermal_hr, self.input_size)\n",
    "        thermal_hr = thermal_hr.astype(np.float32) / 255.0\n",
    "        thermal_hr = torch.from_numpy(thermal_hr).unsqueeze(0)\n",
    "        \n",
    "        label = self.class_to_idx[row['pose_label']]\n",
    "        \n",
    "        return thermal_hr, label\n",
    "\n",
    "print(\"âœ… è³‡æ–™é›†å®šç¾©å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: æ¨¡å‹æ¶æ§‹å®šç¾©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNet åŸºç¤æ¨¡çµ„\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "\n",
    "# éšæ®µ 1: UNet\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.enc1 = DoubleConv(in_channels, 64)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.enc2 = DoubleConv(64, 128)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        self.enc3 = DoubleConv(128, 256)\n",
    "        self.pool3 = nn.MaxPool2d(2)\n",
    "        self.enc4 = DoubleConv(256, 512)\n",
    "        self.pool4 = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.bottleneck = DoubleConv(512, 1024)\n",
    "        \n",
    "        self.upconv4 = nn.ConvTranspose2d(1024, 512, 2, stride=2)\n",
    "        self.dec4 = DoubleConv(1024, 512)\n",
    "        self.upconv3 = nn.ConvTranspose2d(512, 256, 2, stride=2)\n",
    "        self.dec3 = DoubleConv(512, 256)\n",
    "        self.upconv2 = nn.ConvTranspose2d(256, 128, 2, stride=2)\n",
    "        self.dec2 = DoubleConv(256, 128)\n",
    "        self.upconv1 = nn.ConvTranspose2d(128, 64, 2, stride=2)\n",
    "        self.dec1 = DoubleConv(128, 64)\n",
    "        \n",
    "        self.out_conv = nn.Conv2d(64, out_channels, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        enc1 = self.enc1(x)\n",
    "        enc2 = self.enc2(self.pool1(enc1))\n",
    "        enc3 = self.enc3(self.pool2(enc2))\n",
    "        enc4 = self.enc4(self.pool3(enc3))\n",
    "        \n",
    "        bottleneck = self.bottleneck(self.pool4(enc4))\n",
    "        \n",
    "        dec4 = self.upconv4(bottleneck)\n",
    "        dec4 = torch.cat([dec4, enc4], dim=1)\n",
    "        dec4 = self.dec4(dec4)\n",
    "        \n",
    "        dec3 = self.upconv3(dec4)\n",
    "        dec3 = torch.cat([dec3, enc3], dim=1)\n",
    "        dec3 = self.dec3(dec3)\n",
    "        \n",
    "        dec2 = self.upconv2(dec3)\n",
    "        dec2 = torch.cat([dec2, enc2], dim=1)\n",
    "        dec2 = self.dec2(dec2)\n",
    "        \n",
    "        dec1 = self.upconv1(dec2)\n",
    "        dec1 = torch.cat([dec1, enc1], dim=1)\n",
    "        dec1 = self.dec1(dec1)\n",
    "        \n",
    "        return torch.sigmoid(self.out_conv(dec1))\n",
    "\n",
    "\n",
    "# éšæ®µ 2: Semantic-Guided SR\n",
    "class SemanticGuidedSRNet(nn.Module):\n",
    "    def __init__(self, scale_factor=8):\n",
    "        super().__init__()\n",
    "        self.scale_factor = scale_factor\n",
    "        \n",
    "        self.thermal_encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 128, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        \n",
    "        self.mask_encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 64, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        \n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Conv2d(128 + 64, 256, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 128, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        \n",
    "        self.upsample = nn.Sequential(\n",
    "            nn.Conv2d(128, 128 * (scale_factor ** 2), 3, padding=1),\n",
    "            nn.PixelShuffle(scale_factor),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        \n",
    "        self.refine = nn.Sequential(\n",
    "            nn.Conv2d(128, 64, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 32, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 1, 3, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, thermal_lr, person_mask):\n",
    "        thermal_feat = self.thermal_encoder(thermal_lr)\n",
    "        thermal_feat_up = F.interpolate(thermal_feat, size=person_mask.shape[-2:], \n",
    "                                       mode='bilinear', align_corners=False)\n",
    "        \n",
    "        mask_feat = self.mask_encoder(person_mask)\n",
    "        \n",
    "        fused = torch.cat([thermal_feat_up, mask_feat], dim=1)\n",
    "        fused = self.fusion(fused)\n",
    "        \n",
    "        upsampled = self.upsample(F.interpolate(thermal_feat, scale_factor=self.scale_factor, mode='nearest'))\n",
    "        upsampled = F.interpolate(upsampled, size=person_mask.shape[-2:], mode='bilinear', align_corners=False)\n",
    "        \n",
    "        output = self.refine(upsampled)\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "# éšæ®µ 3: Pose Classifier\n",
    "class PoseClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=3):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            \n",
    "            nn.Conv2d(32, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            \n",
    "            nn.Conv2d(64, 128, 3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            \n",
    "            nn.Conv2d(128, 256, 3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "print(\"âœ… æ¨¡å‹å®šç¾©å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: Loss å‡½æ•¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerceptualLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        vgg = models.vgg16(pretrained=True).features[:16].eval()\n",
    "        for param in vgg.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.vgg = vgg\n",
    "        self.criterion = nn.L1Loss()\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        pred_rgb = pred.repeat(1, 3, 1, 1)\n",
    "        target_rgb = target.repeat(1, 3, 1, 1)\n",
    "        \n",
    "        pred_feat = self.vgg(pred_rgb)\n",
    "        target_feat = self.vgg(target_rgb)\n",
    "        \n",
    "        return self.criterion(pred_feat, target_feat)\n",
    "\n",
    "\n",
    "class PersonFocusLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.criterion = nn.L1Loss(reduction='none')\n",
    "    \n",
    "    def forward(self, pred, target, person_mask):\n",
    "        loss = self.criterion(pred, target)\n",
    "        weight = person_mask * 2.0 + 1.0\n",
    "        weighted_loss = loss * weight\n",
    "        return weighted_loss.mean()\n",
    "\n",
    "\n",
    "class CombinedSRLoss(nn.Module):\n",
    "    def __init__(self, lambda_l1=1.0, lambda_perceptual=0.1, lambda_person_focus=0.5):\n",
    "        super().__init__()\n",
    "        self.lambda_l1 = lambda_l1\n",
    "        self.lambda_perceptual = lambda_perceptual\n",
    "        self.lambda_person_focus = lambda_person_focus\n",
    "        \n",
    "        self.l1_loss = nn.L1Loss()\n",
    "        self.perceptual_loss = PerceptualLoss()\n",
    "        self.person_focus_loss = PersonFocusLoss()\n",
    "    \n",
    "    def forward(self, pred, target, person_mask):\n",
    "        l1 = self.l1_loss(pred, target)\n",
    "        perceptual = self.perceptual_loss(pred, target)\n",
    "        person_focus = self.person_focus_loss(pred, target, person_mask)\n",
    "        \n",
    "        total_loss = (\n",
    "            self.lambda_l1 * l1 +\n",
    "            self.lambda_perceptual * perceptual +\n",
    "            self.lambda_person_focus * person_focus\n",
    "        )\n",
    "        \n",
    "        return total_loss, {\n",
    "            'l1': l1.item(),\n",
    "            'perceptual': perceptual.item(),\n",
    "            'person_focus': person_focus.item(),\n",
    "            'total': total_loss.item()\n",
    "        }\n",
    "\n",
    "print(\"âœ… Loss å‡½æ•¸å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 6: è¼‰å…¥ä¸¦åˆ†å‰²è³‡æ–™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¼‰å…¥æ¨™ç±¤\n",
    "labels_df = pd.read_csv(LABELS_CSV)\n",
    "\n",
    "print(f\"\\nç¸½æ¨£æœ¬æ•¸: {len(labels_df):,}\")\n",
    "print(f\"\\nå§¿å‹¢åˆ†ä½ˆ:\")\n",
    "print(labels_df['pose_label'].value_counts())\n",
    "\n",
    "# åˆ†å‰²è³‡æ–™é›†\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_val_df, test_df = train_test_split(\n",
    "    labels_df, test_size=TEST_SPLIT, random_state=SEED,\n",
    "    stratify=labels_df['pose_label']\n",
    ")\n",
    "\n",
    "train_df, val_df = train_test_split(\n",
    "    train_val_df, test_size=VAL_SPLIT/(TRAIN_SPLIT+VAL_SPLIT),\n",
    "    random_state=SEED, stratify=train_val_df['pose_label']\n",
    ")\n",
    "\n",
    "print(f\"\\nè³‡æ–™åˆ†å‰²:\")\n",
    "print(f\"  è¨“ç·´é›†: {len(train_df):,}\")\n",
    "print(f\"  é©—è­‰é›†: {len(val_df):,}\")\n",
    "print(f\"  æ¸¬è©¦é›†: {len(test_df):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 7: éšæ®µ 1 - è¨“ç·´ RGB äººé«”åˆ†å‰²"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_stage1_segmentation(train_df, val_df, config, device):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"éšæ®µ 1: RGB äººé«”åˆ†å‰²è¨“ç·´\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    train_dataset = RGBSegmentationDataset(train_df, RGB_DIR, MASKS_DIR, config['input_size'])\n",
    "    val_dataset = RGBSegmentationDataset(val_df, RGB_DIR, MASKS_DIR, config['input_size'])\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True, num_workers=4)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=False, num_workers=4)\n",
    "    \n",
    "    model = UNet().to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'])\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=5)\n",
    "    \n",
    "    writer = SummaryWriter(LOGS_DIR / config['name'])\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(config['epochs']):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        \n",
    "        for rgb, mask in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{config['epochs']} [Train]\"):\n",
    "            rgb, mask = rgb.to(device), mask.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            pred = model(rgb)\n",
    "            loss = criterion(pred, mask)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for rgb, mask in val_loader:\n",
    "                rgb, mask = rgb.to(device), mask.to(device)\n",
    "                pred = model(rgb)\n",
    "                loss = criterion(pred, mask)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        \n",
    "        writer.add_scalar('Loss/train', train_loss, epoch)\n",
    "        writer.add_scalar('Loss/val', val_loss, epoch)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}: Train={train_loss:.4f}, Val={val_loss:.4f}\")\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), MODELS_DIR / f\"{config['name']}_best.pth\")\n",
    "            print(f\"  âœ“ Best model saved\")\n",
    "    \n",
    "    writer.close()\n",
    "    print(f\"\\nâœ… éšæ®µ 1 å®Œæˆï¼ŒBest Val Loss: {best_val_loss:.4f}\")\n",
    "    return model\n",
    "\n",
    "# åŸ·è¡Œ\n",
    "# segmentation_model = train_stage1_segmentation(train_df, val_df, STAGE1_CONFIG, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 8: éšæ®µ 2 - è¨“ç·´èªç¾©å¼•å°è¶…è§£æåº¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_stage2_sr(train_df, val_df, config, device):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"éšæ®µ 2: èªç¾©å¼•å°è¶…è§£æåº¦è¨“ç·´\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    train_dataset = SemanticGuidedSRDataset(\n",
    "        train_df, THERMAL_DIR, RGB_DIR, MASKS_DIR,\n",
    "        config['thermal_input_size'], config['thermal_output_size']\n",
    "    )\n",
    "    val_dataset = SemanticGuidedSRDataset(\n",
    "        val_df, THERMAL_DIR, RGB_DIR, MASKS_DIR,\n",
    "        config['thermal_input_size'], config['thermal_output_size']\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True, num_workers=4)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=False, num_workers=4)\n",
    "    \n",
    "    model = SemanticGuidedSRNet(scale_factor=config['scale_factor']).to(device)\n",
    "    criterion = CombinedSRLoss(\n",
    "        lambda_l1=config['lambda_l1'],\n",
    "        lambda_perceptual=config['lambda_perceptual'],\n",
    "        lambda_person_focus=config['lambda_person_focus']\n",
    "    ).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'])\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=config['epochs'])\n",
    "    \n",
    "    writer = SummaryWriter(LOGS_DIR / config['name'])\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(config['epochs']):\n",
    "        model.train()\n",
    "        train_losses = defaultdict(float)\n",
    "        \n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{config['epochs']} [Train]\"):\n",
    "            thermal_lr = batch['thermal_lr'].to(device)\n",
    "            person_mask = batch['person_mask'].to(device)\n",
    "            rgb_gray_hr = batch['rgb_gray_hr'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            pred_hr = model(thermal_lr, person_mask)\n",
    "            loss, loss_dict = criterion(pred_hr, rgb_gray_hr, person_mask)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            for k, v in loss_dict.items():\n",
    "                train_losses[k] += v\n",
    "        \n",
    "        for k in train_losses:\n",
    "            train_losses[k] /= len(train_loader)\n",
    "        \n",
    "        model.eval()\n",
    "        val_losses = defaultdict(float)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                thermal_lr = batch['thermal_lr'].to(device)\n",
    "                person_mask = batch['person_mask'].to(device)\n",
    "                rgb_gray_hr = batch['rgb_gray_hr'].to(device)\n",
    "                \n",
    "                pred_hr = model(thermal_lr, person_mask)\n",
    "                _, loss_dict = criterion(pred_hr, rgb_gray_hr, person_mask)\n",
    "                \n",
    "                for k, v in loss_dict.items():\n",
    "                    val_losses[k] += v\n",
    "        \n",
    "        for k in val_losses:\n",
    "            val_losses[k] /= len(val_loader)\n",
    "        \n",
    "        for k in train_losses:\n",
    "            writer.add_scalar(f'Loss/train_{k}', train_losses[k], epoch)\n",
    "            writer.add_scalar(f'Loss/val_{k}', val_losses[k], epoch)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}: Train Total={train_losses['total']:.4f}, Val Total={val_losses['total']:.4f}\")\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        if val_losses['total'] < best_val_loss:\n",
    "            best_val_loss = val_losses['total']\n",
    "            torch.save(model.state_dict(), MODELS_DIR / f\"{config['name']}_best.pth\")\n",
    "            print(f\"  âœ“ Best model saved\")\n",
    "    \n",
    "    writer.close()\n",
    "    print(f\"\\nâœ… éšæ®µ 2 å®Œæˆï¼ŒBest Val Loss: {best_val_loss:.4f}\")\n",
    "    return model\n",
    "\n",
    "# åŸ·è¡Œ\n",
    "# sr_model = train_stage2_sr(train_df, val_df, STAGE2_CONFIG, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 9: ç”Ÿæˆé«˜è§£æåº¦ç†±æˆåƒï¼ˆéšæ®µ 3 æº–å‚™ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_thermal_hr(labels_df, sr_model, output_dir, device):\n",
    "    print(f\"\\nç”Ÿæˆé«˜è§£æåº¦ç†±æˆåƒ...\")\n",
    "    \n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "    sr_model.eval()\n",
    "    \n",
    "    dataset = SemanticGuidedSRDataset(\n",
    "        labels_df, THERMAL_DIR, RGB_DIR, MASKS_DIR,\n",
    "        (24, 32), (256, 256)\n",
    "    )\n",
    "    loader = DataLoader(dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc=\"ç”Ÿæˆ Thermal HR\"):\n",
    "            thermal_lr = batch['thermal_lr'].to(device)\n",
    "            person_mask = batch['person_mask'].to(device)\n",
    "            \n",
    "            thermal_hr = sr_model(thermal_lr, person_mask)\n",
    "            \n",
    "            for i, pair_id in enumerate(batch['pair_id']):\n",
    "                hr_img = thermal_hr[i, 0].cpu().numpy()\n",
    "                hr_img = (hr_img * 255).astype(np.uint8)\n",
    "                \n",
    "                save_path = output_dir / f\"{pair_id}_thermal_hr.npy\"\n",
    "                np.save(save_path, hr_img)\n",
    "    \n",
    "    print(f\"âœ… å®Œæˆï¼Œå„²å­˜æ–¼ {output_dir}\")\n",
    "\n",
    "# åŸ·è¡Œ\n",
    "# THERMAL_HR_DIR = ALIGNED_DIR / 'thermal_hr'\n",
    "# generate_thermal_hr(labels_df, sr_model, THERMAL_HR_DIR, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 10: éšæ®µ 3 - è¨“ç·´å§¿å‹¢è¾¨è­˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_stage3_pose_classifier(train_df, val_df, config, thermal_hr_dir, device):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"éšæ®µ 3: å§¿å‹¢è¾¨è­˜è¨“ç·´\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    train_dataset = PoseClassificationDataset(train_df, thermal_hr_dir)\n",
    "    val_dataset = PoseClassificationDataset(val_df, thermal_hr_dir)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True, num_workers=4)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=False, num_workers=4)\n",
    "    \n",
    "    model = PoseClassifier(num_classes=config['num_classes']).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'])\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=15, gamma=0.1)\n",
    "    \n",
    "    writer = SummaryWriter(LOGS_DIR / config['name'])\n",
    "    \n",
    "    best_val_acc = 0\n",
    "    \n",
    "    for epoch in range(config['epochs']):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        for thermal_hr, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{config['epochs']} [Train]\"):\n",
    "            thermal_hr, labels = thermal_hr.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(thermal_hr)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        train_acc = 100. * train_correct / train_total\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for thermal_hr, labels in val_loader:\n",
    "                thermal_hr, labels = thermal_hr.to(device), labels.to(device)\n",
    "                outputs = model(thermal_hr)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        val_acc = 100. * val_correct / val_total\n",
    "        \n",
    "        writer.add_scalar('Loss/train', train_loss, epoch)\n",
    "        writer.add_scalar('Loss/val', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/train', train_acc, epoch)\n",
    "        writer.add_scalar('Accuracy/val', val_acc, epoch)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}: Train Acc={train_acc:.2f}%, Val Acc={val_acc:.2f}%\")\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), MODELS_DIR / f\"{config['name']}_best.pth\")\n",
    "            print(f\"  âœ“ Best model saved\")\n",
    "    \n",
    "    writer.close()\n",
    "    print(f\"\\nâœ… éšæ®µ 3 å®Œæˆï¼ŒBest Val Acc: {best_val_acc:.2f}%\")\n",
    "    return model\n",
    "\n",
    "# åŸ·è¡Œ\n",
    "# pose_model = train_stage3_pose_classifier(train_df, val_df, STAGE3_CONFIG, THERMAL_HR_DIR, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 11: å®Œæ•´è¨“ç·´æµç¨‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"é–‹å§‹å®Œæ•´è¨“ç·´æµç¨‹\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "# éšæ®µ 1\n",
    "print(\"\\nğŸ”„ éšæ®µ 1: RGB äººé«”åˆ†å‰²...\")\n",
    "segmentation_model = train_stage1_segmentation(train_df, val_df, STAGE1_CONFIG, device)\n",
    "\n",
    "# éšæ®µ 2\n",
    "print(\"\\nğŸ”„ éšæ®µ 2: èªç¾©å¼•å°è¶…è§£æåº¦...\")\n",
    "sr_model = train_stage2_sr(train_df, val_df, STAGE2_CONFIG, device)\n",
    "\n",
    "# ç”Ÿæˆ Thermal HR\n",
    "THERMAL_HR_DIR = ALIGNED_DIR / 'thermal_hr'\n",
    "generate_thermal_hr(labels_df, sr_model, THERMAL_HR_DIR, device)\n",
    "\n",
    "# éšæ®µ 3\n",
    "print(\"\\nğŸ”„ éšæ®µ 3: å§¿å‹¢è¾¨è­˜...\")\n",
    "pose_model = train_stage3_pose_classifier(train_df, val_df, STAGE3_CONFIG, THERMAL_HR_DIR, device)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"âœ… æ‰€æœ‰éšæ®µè¨“ç·´å®Œæˆï¼\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"\\nè¨“ç·´æ¨¡å‹:\")\n",
    "print(f\"  - {MODELS_DIR / 'RGB_Segmentation_best.pth'}\")\n",
    "print(f\"  - {MODELS_DIR / 'Semantic_Guided_SR_best.pth'}\")\n",
    "print(f\"  - {MODELS_DIR / 'Pose_Classifier_best.pth'}\")\n",
    "print(f\"\\nTensorBoard logs:\")\n",
    "print(f\"  tensorboard --logdir={LOGS_DIR}\")\n",
    "print(f\"\\n{'='*70}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
