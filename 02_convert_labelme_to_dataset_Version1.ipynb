{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ç—…åºŠå§¿å‹¢ç›£æ¸¬ - LabelMe æ¨™è¨»è½‰æ›\n",
    "\n",
    "**å°ˆæ¡ˆ**: ç†±æˆåƒè¶…è§£æåº¦ + å§¿å‹¢è¾¨è­˜  \n",
    "**ä½œè€…**: rochi190  \n",
    "**æ—¥æœŸ**: 2025-11-03  \n",
    "\n",
    "## åŠŸèƒ½\n",
    "1. è®€å– LabelMe JSON æ¨™è¨»\n",
    "2. è§£æäººé«”å€åŸŸ polygon\n",
    "3. ç”Ÿæˆ binary maskï¼ˆäººé«”å€åŸŸï¼‰\n",
    "4. å»ºç«‹è¨“ç·´ç”¨ CSVï¼ˆpair_id, pose_label, mask_pathï¼‰\n",
    "5. çµ±è¨ˆæ¨™è¨»åˆ†ä½ˆ\n",
    "6. è¦–è¦ºåŒ–æ¨™è¨»çµæœ\n",
    "\n",
    "## è¼¸å…¥\n",
    "- `output/labelme_project/annotations/` - LabelMe JSON æª”æ¡ˆ\n",
    "- `output/labelme_project/image_metadata.csv` - åœ–ç‰‡ metadata\n",
    "\n",
    "## è¼¸å‡º\n",
    "- `output/training_dataset/person_masks/` - äººé«” mask (PNG)\n",
    "- `output/training_dataset/pose_labels.csv` - å§¿å‹¢æ¨™ç±¤\n",
    "- `output/training_dataset/label_statistics.png` - çµ±è¨ˆåœ–è¡¨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 0: ç’°å¢ƒè¨­å®šèˆ‡å¥—ä»¶åŒ¯å…¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ç—…åºŠå§¿å‹¢ç›£æ¸¬ç³»çµ± - LabelMe æ¨™è¨»è½‰æ›æ¨¡çµ„\n",
      "======================================================================\n",
      "åŸ·è¡Œæ™‚é–“: 2025-11-04 18:15:23\n",
      "ä½¿ç”¨è€…: rochi190\n",
      "Python ç‰ˆæœ¬: 3.11.13 (main, Jun  5 2025, 13:12:00) [GCC 11.2.0]\n",
      "NumPy ç‰ˆæœ¬: 1.26.4\n",
      "OpenCV ç‰ˆæœ¬: 4.10.0\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple, Optional, Any\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from collections import Counter, defaultdict\n",
    "# Matplotlib è¨­å®š\n",
    "plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei', 'DejaVu Sans']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "# Seaborn æ¨£å¼\n",
    "sns.set_style('whitegrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ç—…åºŠå§¿å‹¢ç›£æ¸¬ç³»çµ± - LabelMe æ¨™è¨»è½‰æ›æ¨¡çµ„\")\n",
    "print(\"=\"*70)\n",
    "print(f\"åŸ·è¡Œæ™‚é–“: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"ä½¿ç”¨è€…: rochi190\")\n",
    "print(f\"Python ç‰ˆæœ¬: {sys.version}\")\n",
    "print(f\"NumPy ç‰ˆæœ¬: {np.__version__}\")\n",
    "print(f\"OpenCV ç‰ˆæœ¬: {cv2.__version__}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: è·¯å¾‘é…ç½®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ·ï¸ è¨“ç·´ç”¨å§¿å‹¢é¡åˆ¥:\n",
      "  0. lying\n",
      "  1. sitting\n",
      "  2. fallen\n",
      "\n",
      "âš ï¸ éæ¿¾çš„é¡åˆ¥: empty, uncertain\n",
      "\n",
      "ğŸ“ è·¯å¾‘æª¢æŸ¥:\n",
      "  LabelMe ç›®éŒ„: âœ“ /home/gary/claude4.5/output/labelme_project\n",
      "  åœ–ç‰‡ç›®éŒ„: âœ“ /home/gary/claude4.5/output/labelme_project/images\n",
      "  æ¨™è¨»ç›®éŒ„: âœ“ /home/gary/claude4.5/output/labelme_project/annotations\n",
      "  Metadata: âœ— /home/gary/claude4.5/output/labelme_project/image_metadata.csv\n",
      "\n",
      "ğŸ·ï¸ å§¿å‹¢é¡åˆ¥:\n",
      "  0. lying\n",
      "  1. sitting\n",
      "  2. fallen\n",
      "\n",
      "ğŸ“Š æ¨™è¨»æª”æ¡ˆ: 0 å€‹\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# è·¯å¾‘é…ç½®\n",
    "# ========================================\n",
    "BASE_DIR = Path(os.getcwd())\n",
    "OUTPUT_DIR = BASE_DIR / 'output'\n",
    "\n",
    "# LabelMe å°ˆæ¡ˆç›®éŒ„\n",
    "LABELME_DIR = OUTPUT_DIR / 'labelme_project'\n",
    "IMAGES_DIR = LABELME_DIR / 'images'\n",
    "ANNOTATIONS_DIR = LABELME_DIR / 'annotations'\n",
    "METADATA_CSV = LABELME_DIR / 'image_metadata.csv'\n",
    "\n",
    "# è¼¸å‡ºç›®éŒ„\n",
    "TRAINING_DIR = OUTPUT_DIR / 'training_dataset'\n",
    "MASKS_DIR = TRAINING_DIR / 'person_masks'\n",
    "\n",
    "# ğŸ”§ æ”¹ç‚º adaptive_pairing\n",
    "ADAPTIVE_DIR = OUTPUT_DIR / 'adaptive_pairing'  # â† é€™è£¡\n",
    "ALIGNED_DIR = OUTPUT_DIR / 'aligned_dataset'    # â† ä¿ç•™ï¼ˆç”¨æ–¼ 03 è¨“ç·´æ™‚ï¼‰\n",
    "\n",
    "# å»ºç«‹è¼¸å‡ºç›®éŒ„\n",
    "for directory in [TRAINING_DIR, MASKS_DIR]:\n",
    "    directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ========================================\n",
    "# é¡åˆ¥æ˜ å°„ï¼ˆåªä¿ç•™ 3 å€‹æœ‰æ•ˆå§¿å‹¢ï¼‰\n",
    "# ========================================\n",
    "CLASS_MAPPING = {\n",
    "    'lying': 0,      # èººè‘—\n",
    "    'sitting': 1,    # åè‘—\n",
    "    'fallen': 2,     # è·Œå€’\n",
    "    # 'empty': ç§»é™¤ï¼ˆè¦–é‡å•é¡Œï¼‰\n",
    "    # 'uncertain': ç§»é™¤ï¼ˆå“è³ªä¸ä½³ï¼‰\n",
    "}\n",
    "\n",
    "CLASS_NAMES = list(CLASS_MAPPING.keys())\n",
    "VALID_POSES = ['lying', 'sitting', 'fallen']\n",
    "\n",
    "print(f\"\\nğŸ·ï¸ è¨“ç·´ç”¨å§¿å‹¢é¡åˆ¥:\")\n",
    "for name, idx in CLASS_MAPPING.items():\n",
    "    print(f\"  {idx}. {name}\")\n",
    "\n",
    "print(f\"\\nâš ï¸ éæ¿¾çš„é¡åˆ¥: empty, uncertain\")\n",
    "\n",
    "# ========================================\n",
    "# é¡¯ç¤ºé…ç½®\n",
    "# ========================================\n",
    "print(f\"\\nğŸ“ è·¯å¾‘æª¢æŸ¥:\")\n",
    "print(f\"  LabelMe ç›®éŒ„: {'âœ“' if LABELME_DIR.exists() else 'âœ—'} {LABELME_DIR}\")\n",
    "print(f\"  åœ–ç‰‡ç›®éŒ„: {'âœ“' if IMAGES_DIR.exists() else 'âœ—'} {IMAGES_DIR}\")\n",
    "print(f\"  æ¨™è¨»ç›®éŒ„: {'âœ“' if ANNOTATIONS_DIR.exists() else 'âœ—'} {ANNOTATIONS_DIR}\")\n",
    "print(f\"  Metadata: {'âœ“' if METADATA_CSV.exists() else 'âœ—'} {METADATA_CSV}\")\n",
    "print(f\"  Adaptive Pairing: {'âœ“' if ADAPTIVE_DIR.exists() else 'âœ—'} {ADAPTIVE_DIR}\")\n",
    "\n",
    "print(f\"\\nğŸ·ï¸ å§¿å‹¢é¡åˆ¥:\")\n",
    "for name, idx in CLASS_MAPPING.items():\n",
    "    print(f\"  {idx}. {name}\")\n",
    "\n",
    "# æª¢æŸ¥æ¨™è¨»æª”æ¡ˆ\n",
    "if ANNOTATIONS_DIR.exists():\n",
    "    json_files = list(ANNOTATIONS_DIR.glob('*.json'))\n",
    "    print(f\"\\nğŸ“Š æ¨™è¨»æª”æ¡ˆ: {len(json_files)} å€‹\")\n",
    "else:\n",
    "    print(f\"\\nâš ï¸ æ¨™è¨»ç›®éŒ„ä¸å­˜åœ¨ï¼Œè«‹å…ˆåŸ·è¡Œ LabelMe æ¨™è¨»\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: è§£æ LabelMe JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_labelme_json(json_path: Path) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    è§£æ LabelMe JSON æª”æ¡ˆ\n",
    "    \n",
    "    Args:\n",
    "        json_path: JSON æª”æ¡ˆè·¯å¾‘\n",
    "        \n",
    "    Returns:\n",
    "        åŒ…å«æ¨™è¨»è³‡è¨Šçš„å­—å…¸\n",
    "    \"\"\"\n",
    "    with open(json_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    labels = []\n",
    "    polygons = []\n",
    "    \n",
    "    for shape in data.get('shapes', []):\n",
    "        label = shape['label']\n",
    "        points = np.array(shape['points'], dtype=np.int32)\n",
    "        \n",
    "        labels.append(label)\n",
    "        polygons.append(points)\n",
    "    \n",
    "    return {\n",
    "        'image_path': data['imagePath'],\n",
    "        'labels': labels,\n",
    "        'polygons': polygons,\n",
    "        'image_height': data['imageHeight'],\n",
    "        'image_width': data['imageWidth'],\n",
    "        'version': data.get('version', 'unknown')\n",
    "    }\n",
    "\n",
    "def create_mask_from_polygon(points: np.ndarray, height: int, width: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    å¾ polygon é»å»ºç«‹ binary mask\n",
    "    \n",
    "    Args:\n",
    "        points: Polygon é ‚é»åº§æ¨™ (N, 2)\n",
    "        height: åœ–ç‰‡é«˜åº¦\n",
    "        width: åœ–ç‰‡å¯¬åº¦\n",
    "        \n",
    "    Returns:\n",
    "        Binary mask (uint8, 0 æˆ– 255)\n",
    "    \"\"\"\n",
    "    mask = np.zeros((height, width), dtype=np.uint8)\n",
    "    cv2.fillPoly(mask, [points], 255)\n",
    "    return mask\n",
    "\n",
    "# æ¸¬è©¦è§£æåŠŸèƒ½\n",
    "json_files = sorted(ANNOTATIONS_DIR.glob('*.json'))\n",
    "\n",
    "if json_files:\n",
    "    print(f\"\\næ¸¬è©¦è§£æç¬¬ä¸€å€‹æ¨™è¨»æª”æ¡ˆ...\")\n",
    "    test_annotation = parse_labelme_json(json_files[0])\n",
    "    \n",
    "    print(f\"\\nâœ… è§£ææˆåŠŸ\")\n",
    "    print(f\"  åœ–ç‰‡: {test_annotation['image_path']}\")\n",
    "    print(f\"  è§£æåº¦: {test_annotation['image_width']} Ã— {test_annotation['image_height']}\")\n",
    "    print(f\"  æ¨™è¨»æ•¸é‡: {len(test_annotation['labels'])}\")\n",
    "    print(f\"  æ¨™ç±¤: {test_annotation['labels']}\")\n",
    "    print(f\"  LabelMe ç‰ˆæœ¬: {test_annotation['version']}\")\n",
    "else:\n",
    "    print(f\"\\nâŒ æœªæ‰¾åˆ°æ¨™è¨»æª”æ¡ˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: æ‰¹æ¬¡è½‰æ›æ¨™è¨»"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_all_annotations(\n",
    "    annotations_dir: Path,\n",
    "    metadata_csv: Path,\n",
    "    output_masks_dir: Path\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    è½‰æ›æ‰€æœ‰ LabelMe æ¨™è¨»ç‚ºè¨“ç·´æ ¼å¼\n",
    "    \n",
    "    ä¿®æ”¹ï¼šéæ¿¾ empty å’Œ uncertain æ¨™è¨»\n",
    "    \"\"\"\n",
    "    json_files = sorted(annotations_dir.glob('*.json'))\n",
    "    \n",
    "    if not json_files:\n",
    "        raise FileNotFoundError(f\"âŒ æœªæ‰¾åˆ°æ¨™è¨»æª”æ¡ˆæ–¼ {annotations_dir}\")\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"è½‰æ› LabelMe æ¨™è¨»\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"æ‰¾åˆ° {len(json_files)} å€‹æ¨™è¨»æª”æ¡ˆ\")\n",
    "    \n",
    "    # è¼‰å…¥ metadata\n",
    "    if metadata_csv.exists():\n",
    "        metadata_df = pd.read_csv(metadata_csv)\n",
    "        print(f\"è¼‰å…¥ metadata: {len(metadata_df)} ç­†\")\n",
    "    else:\n",
    "        print(f\"âš ï¸ æœªæ‰¾åˆ° metadata\")\n",
    "        metadata_df = None\n",
    "    \n",
    "    results = []\n",
    "    skipped = []\n",
    "    skipped_by_pose = defaultdict(int)  # çµ±è¨ˆå„é¡åˆ¥è¢«è·³éçš„æ•¸é‡\n",
    "    \n",
    "    print(f\"\\né–‹å§‹è½‰æ›ï¼ˆåªä¿ç•™ {VALID_POSES}ï¼‰...\")\n",
    "    for json_path in tqdm(json_files, desc=\"è½‰æ›æ¨™è¨»\", unit=\"file\"):\n",
    "        try:\n",
    "            # è§£æ JSON\n",
    "            annotation = parse_labelme_json(json_path)\n",
    "            \n",
    "            # æ‰¾å°æ‡‰çš„ metadata\n",
    "            image_name = Path(annotation['image_path']).name\n",
    "            \n",
    "            if metadata_df is not None:\n",
    "                meta_row = metadata_df[metadata_df['filename'] == image_name]\n",
    "                \n",
    "                if meta_row.empty:\n",
    "                    skipped.append({'file': json_path.name, 'reason': 'metadata not found'})\n",
    "                    continue\n",
    "                \n",
    "                pair_id = meta_row.iloc[0]['pair_id']\n",
    "                timestamp = meta_row.iloc[0]['timestamp']\n",
    "                rgb_frame_idx = meta_row.iloc[0]['rgb_frame_idx']\n",
    "            else:\n",
    "                # å¾æª”åæ¨æ–· pair_id\n",
    "                pair_id = image_name.replace('frame_', '').replace('.jpg', '')\n",
    "                pair_id = f\"pair_{pair_id}\"\n",
    "                timestamp = None\n",
    "                rgb_frame_idx = None\n",
    "            \n",
    "            # è™•ç†æ¯å€‹æ¨™è¨»ï¼ˆé€šå¸¸åªæœ‰ä¸€å€‹äººï¼‰\n",
    "            for idx, (label, polygon) in enumerate(zip(annotation['labels'], annotation['polygons'])):\n",
    "                \n",
    "                # ğŸ”§ éæ¿¾ç„¡æ•ˆå§¿å‹¢\n",
    "                if label not in VALID_POSES:\n",
    "                    skipped_by_pose[label] += 1\n",
    "                    skipped.append({\n",
    "                        'file': json_path.name, \n",
    "                        'reason': f'filtered pose: {label}'\n",
    "                    })\n",
    "                    continue\n",
    "                \n",
    "                # å»ºç«‹ mask\n",
    "                mask = create_mask_from_polygon(\n",
    "                    polygon,\n",
    "                    annotation['image_height'],\n",
    "                    annotation['image_width']\n",
    "                )\n",
    "                \n",
    "                # å„²å­˜ mask\n",
    "                mask_filename = f\"{pair_id}_mask.png\"\n",
    "                mask_path = output_masks_dir / mask_filename\n",
    "                cv2.imwrite(str(mask_path), mask)\n",
    "                \n",
    "                # è¨˜éŒ„çµæœ\n",
    "                results.append({\n",
    "                    'pair_id': pair_id,\n",
    "                    'timestamp': timestamp,\n",
    "                    'rgb_frame_idx': rgb_frame_idx,\n",
    "                    'image_file': image_name,\n",
    "                    'mask_file': mask_filename,\n",
    "                    'mask_path': str(mask_path.relative_to(output_masks_dir.parent)),\n",
    "                    'pose_label': label,\n",
    "                    'pose_class': CLASS_MAPPING.get(label, -1),\n",
    "                    'polygon_points': len(polygon),\n",
    "                    'mask_area': np.sum(mask > 0),\n",
    "                    'image_width': annotation['image_width'],\n",
    "                    'image_height': annotation['image_height']\n",
    "                })\n",
    "        \n",
    "        except Exception as e:\n",
    "            skipped.append({'file': json_path.name, 'reason': str(e)})\n",
    "    \n",
    "    # å»ºç«‹ DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    print(f\"\\nâœ… è½‰æ›å®Œæˆ\")\n",
    "    print(f\"  æˆåŠŸ: {len(results)} å€‹æ¨™è¨»\")\n",
    "    print(f\"  è·³é: {len(skipped)} å€‹æª”æ¡ˆ\")\n",
    "    \n",
    "    # ğŸ”§ é¡¯ç¤ºéæ¿¾çµ±è¨ˆ\n",
    "    if skipped_by_pose:\n",
    "        print(f\"\\nâš ï¸ ä¾å§¿å‹¢éæ¿¾çµ±è¨ˆ:\")\n",
    "        for pose, count in sorted(skipped_by_pose.items()):\n",
    "            print(f\"    - {pose}: {count} å€‹\")\n",
    "    \n",
    "    if skipped:\n",
    "        print(f\"\\n  å…¶ä»–è·³éåŸå› ï¼ˆå‰ 5 å€‹ï¼‰:\")\n",
    "        for item in skipped[:5]:\n",
    "            if not item['reason'].startswith('filtered pose'):\n",
    "                print(f\"    - {item['file']}: {item['reason']}\")\n",
    "        if len(skipped) > 5:\n",
    "            other_skipped = [s for s in skipped if not s['reason'].startswith('filtered pose')]\n",
    "            if len(other_skipped) > 5:\n",
    "                print(f\"    ... é‚„æœ‰ {len(other_skipped)-5} å€‹\")\n",
    "    \n",
    "    print(f\"\\nğŸ“Š æœ‰æ•ˆæ¨™è¨»åˆ†ä½ˆ:\")\n",
    "    if len(results_df) > 0:\n",
    "        print(results_df['pose_label'].value_counts())\n",
    "    else:\n",
    "        print(\"  âš ï¸ ç„¡æœ‰æ•ˆæ¨™è¨»\")\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "# ========================================\n",
    "# åŸ·è¡Œè½‰æ›\n",
    "# ========================================\n",
    "labels_df = convert_all_annotations(ANNOTATIONS_DIR, METADATA_CSV, MASKS_DIR)\n",
    "\n",
    "# å„²å­˜çµæœ\n",
    "output_csv = TRAINING_DIR / 'pose_labels.csv'\n",
    "labels_df.to_csv(output_csv, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"\\nğŸ’¾ æ¨™ç±¤å·²å„²å­˜: {output_csv}\")\n",
    "print(f\"\\nå‰ 5 ç­†è³‡æ–™:\")\n",
    "print(labels_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: çµ±è¨ˆåˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_label_distribution(labels_df: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    åˆ†ææ¨™è¨»åˆ†ä½ˆ\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"æ¨™è¨»çµ±è¨ˆåˆ†æ\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # åŸºæœ¬çµ±è¨ˆ\n",
    "    print(f\"\\nğŸ“Š åŸºæœ¬çµ±è¨ˆ:\")\n",
    "    print(f\"  ç¸½æ¨™è¨»æ•¸: {len(labels_df):,}\")\n",
    "    print(f\"  å”¯ä¸€ pair æ•¸: {labels_df['pair_id'].nunique():,}\")\n",
    "    \n",
    "    # å§¿å‹¢åˆ†ä½ˆ\n",
    "    print(f\"\\nğŸ·ï¸ å§¿å‹¢åˆ†ä½ˆ:\")\n",
    "    pose_counts = labels_df['pose_label'].value_counts()\n",
    "    for pose, count in pose_counts.items():\n",
    "        percentage = count / len(labels_df) * 100\n",
    "        print(f\"  {pose:12s}: {count:5d} ({percentage:5.1f}%)\")\n",
    "    \n",
    "    # Mask å€åŸŸçµ±è¨ˆ\n",
    "    if 'mask_area' in labels_df.columns:\n",
    "        print(f\"\\nğŸ“ Mask å€åŸŸçµ±è¨ˆ (pixels):\")\n",
    "        total_pixels = labels_df['image_width'] * labels_df['image_height']\n",
    "        mask_ratio = labels_df['mask_area'] / total_pixels * 100\n",
    "        \n",
    "        print(f\"  å¹³å‡é¢ç©: {labels_df['mask_area'].mean():,.0f} pixels\")\n",
    "        print(f\"  ä¸­ä½æ•¸: {labels_df['mask_area'].median():,.0f} pixels\")\n",
    "        print(f\"  æœ€å°: {labels_df['mask_area'].min():,.0f} pixels\")\n",
    "        print(f\"  æœ€å¤§: {labels_df['mask_area'].max():,.0f} pixels\")\n",
    "        print(f\"  å¹³å‡ä½”æ¯”: {mask_ratio.mean():.1f}% of image\")\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "\n",
    "# åŸ·è¡Œåˆ†æ\n",
    "analyze_label_distribution(labels_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: è¦–è¦ºåŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_label_statistics(labels_df: pd.DataFrame, save_dir: Path) -> None:\n",
    "    \"\"\"\n",
    "    è¦–è¦ºåŒ–æ¨™è¨»çµ±è¨ˆ\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # 1. å§¿å‹¢åˆ†ä½ˆï¼ˆæŸ±ç‹€åœ–ï¼‰\n",
    "    pose_counts = labels_df['pose_label'].value_counts()\n",
    "    axes[0, 0].bar(range(len(pose_counts)), pose_counts.values, color='steelblue', edgecolor='black')\n",
    "    axes[0, 0].set_xticks(range(len(pose_counts)))\n",
    "    axes[0, 0].set_xticklabels(pose_counts.index, rotation=45, ha='right')\n",
    "    axes[0, 0].set_ylabel('Count')\n",
    "    axes[0, 0].set_title('Pose Label Distribution (Bar Chart)')\n",
    "    axes[0, 0].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # åŠ ä¸Šæ•¸å€¼æ¨™ç±¤\n",
    "    for i, v in enumerate(pose_counts.values):\n",
    "        axes[0, 0].text(i, v + 0.5, str(v), ha='center', fontweight='bold')\n",
    "    \n",
    "    # 2. å§¿å‹¢åˆ†ä½ˆï¼ˆåœ“é¤…åœ–ï¼‰\n",
    "    axes[0, 1].pie(pose_counts.values, labels=pose_counts.index, autopct='%1.1f%%', startangle=90)\n",
    "    axes[0, 1].set_title('Pose Label Distribution (Pie Chart)')\n",
    "    \n",
    "    # 3. Mask é¢ç©åˆ†ä½ˆ\n",
    "    if 'mask_area' in labels_df.columns:\n",
    "        axes[1, 0].hist(labels_df['mask_area'], bins=30, color='coral', edgecolor='black', alpha=0.7)\n",
    "        axes[1, 0].axvline(labels_df['mask_area'].mean(), color='red', linestyle='--', \n",
    "                          label=f\"Mean: {labels_df['mask_area'].mean():.0f}\")\n",
    "        axes[1, 0].axvline(labels_df['mask_area'].median(), color='green', linestyle='--', \n",
    "                          label=f\"Median: {labels_df['mask_area'].median():.0f}\")\n",
    "        axes[1, 0].set_xlabel('Mask Area (pixels)')\n",
    "        axes[1, 0].set_ylabel('Frequency')\n",
    "        axes[1, 0].set_title('Mask Area Distribution')\n",
    "        axes[1, 0].legend()\n",
    "        axes[1, 0].grid(alpha=0.3)\n",
    "    \n",
    "    # 4. å„å§¿å‹¢çš„ Mask é¢ç©ç®±å‹åœ–\n",
    "    if 'mask_area' in labels_df.columns:\n",
    "        pose_labels = labels_df['pose_label'].unique()\n",
    "        data_for_box = [labels_df[labels_df['pose_label'] == pose]['mask_area'].values \n",
    "                       for pose in pose_labels]\n",
    "        \n",
    "        bp = axes[1, 1].boxplot(data_for_box, labels=pose_labels, patch_artist=True)\n",
    "        for patch in bp['boxes']:\n",
    "            patch.set_facecolor('lightblue')\n",
    "        axes[1, 1].set_ylabel('Mask Area (pixels)')\n",
    "        axes[1, 1].set_title('Mask Area by Pose Label')\n",
    "        axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "        axes[1, 1].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    save_path = save_dir / 'label_statistics.png'\n",
    "    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "    print(f\"\\nâœ… çµ±è¨ˆåœ–è¡¨å·²å„²å­˜: {save_path}\")\n",
    "    plt.show()\n",
    "\n",
    "def visualize_sample_masks(\n",
    "    labels_df: pd.DataFrame,\n",
    "    images_dir: Path,\n",
    "    masks_dir: Path,\n",
    "    save_dir: Path,\n",
    "    num_samples: int = 8\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    è¦–è¦ºåŒ–æ¨£æœ¬ mask\n",
    "    \"\"\"\n",
    "    # æ¯å€‹é¡åˆ¥å–æ¨£\n",
    "    pose_labels = labels_df['pose_label'].unique()\n",
    "    samples_per_class = max(1, num_samples // len(pose_labels))\n",
    "    \n",
    "    sample_rows = []\n",
    "    for pose in pose_labels:\n",
    "        pose_samples = labels_df[labels_df['pose_label'] == pose].sample(\n",
    "            min(samples_per_class, len(labels_df[labels_df['pose_label'] == pose]))\n",
    "        )\n",
    "        sample_rows.append(pose_samples)\n",
    "    \n",
    "    samples_df = pd.concat(sample_rows)\n",
    "    \n",
    "    n_samples = len(samples_df)\n",
    "    n_cols = 4\n",
    "    n_rows = (n_samples + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols * 2, figsize=(16, n_rows * 3))\n",
    "    if n_rows == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    for idx, (_, row) in enumerate(samples_df.iterrows()):\n",
    "        row_idx = idx // n_cols\n",
    "        col_idx = (idx % n_cols) * 2\n",
    "        \n",
    "        # è®€å– RGB åœ–ç‰‡\n",
    "        rgb_path = images_dir / row['image_file']\n",
    "        if rgb_path.exists():\n",
    "            rgb = cv2.imread(str(rgb_path))\n",
    "            rgb = cv2.cvtColor(rgb, cv2.COLOR_BGR2RGB)\n",
    "        else:\n",
    "            rgb = np.zeros((480, 640, 3), dtype=np.uint8)\n",
    "        \n",
    "        # è®€å– mask\n",
    "        mask_path = masks_dir / row['mask_file']\n",
    "        if mask_path.exists():\n",
    "            mask = cv2.imread(str(mask_path), cv2.IMREAD_GRAYSCALE)\n",
    "        else:\n",
    "            mask = np.zeros((480, 640), dtype=np.uint8)\n",
    "        \n",
    "        # é¡¯ç¤º RGB\n",
    "        axes[row_idx, col_idx].imshow(rgb)\n",
    "        axes[row_idx, col_idx].set_title(f\"{row['pose_label']}\\n{row['pair_id']}\")\n",
    "        axes[row_idx, col_idx].axis('off')\n",
    "        \n",
    "        # é¡¯ç¤º Mask overlay\n",
    "        overlay = rgb.copy()\n",
    "        overlay[mask > 0] = [255, 0, 0]  # ç´…è‰²\n",
    "        blended = cv2.addWeighted(rgb, 0.7, overlay, 0.3, 0)\n",
    "        \n",
    "        axes[row_idx, col_idx + 1].imshow(blended)\n",
    "        axes[row_idx, col_idx + 1].set_title(f\"Mask Overlay\\nArea: {row['mask_area']:,.0f}px\")\n",
    "        axes[row_idx, col_idx + 1].axis('off')\n",
    "    \n",
    "    # éš±è—å¤šé¤˜çš„ subplot\n",
    "    for idx in range(n_samples, n_rows * n_cols):\n",
    "        row_idx = idx // n_cols\n",
    "        col_idx = (idx % n_cols) * 2\n",
    "        axes[row_idx, col_idx].axis('off')\n",
    "        axes[row_idx, col_idx + 1].axis('off')\n",
    "    \n",
    "    plt.suptitle('Sample Annotations with Mask Overlay', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    save_path = save_dir / 'sample_masks.png'\n",
    "    plt.savefig(save_path, dpi=120, bbox_inches='tight')\n",
    "    print(f\"\\nâœ… æ¨£æœ¬ Mask å·²å„²å­˜: {save_path}\")\n",
    "    plt.show()\n",
    "\n",
    "# ========================================\n",
    "# åŸ·è¡Œè¦–è¦ºåŒ–\n",
    "# ========================================\n",
    "visualize_label_statistics(labels_df, TRAINING_DIR)\n",
    "visualize_sample_masks(labels_df, IMAGES_DIR, MASKS_DIR, TRAINING_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 6: åˆä½µå°é½Šè³‡æ–™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_with_aligned_data(\n",
    "    labels_df: pd.DataFrame,\n",
    "    aligned_dir: Path\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    åˆä½µæ¨™ç±¤èˆ‡å°é½Šå¾Œçš„ Thermal è³‡æ–™\n",
    "    \n",
    "    Args:\n",
    "        labels_df: æ¨™ç±¤ DataFrame\n",
    "        aligned_dir: å°é½Šè³‡æ–™ç›®éŒ„\n",
    "        \n",
    "    Returns:\n",
    "        åˆä½µå¾Œçš„ DataFrame\n",
    "    \"\"\"\n",
    "    aligned_metadata = aligned_dir / 'metadata.csv'\n",
    "    \n",
    "    if not aligned_metadata.exists():\n",
    "        print(f\"âš ï¸ æœªæ‰¾åˆ°å°é½Šè³‡æ–™: {aligned_metadata}\")\n",
    "        return labels_df\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"åˆä½µå°é½Šè³‡æ–™\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    aligned_df = pd.read_csv(aligned_metadata)\n",
    "    print(f\"è¼‰å…¥å°é½Šè³‡æ–™: {len(aligned_df)} ç­†\")\n",
    "    \n",
    "    # åˆä½µ\n",
    "    merged_df = labels_df.merge(\n",
    "        aligned_df[['pair_id', 'thermal_file', 'thermal_error_ms', 'rgb_error_ms']],\n",
    "        on='pair_id',\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nâœ… åˆä½µå®Œæˆ\")\n",
    "    print(f\"  æ¨™ç±¤æ•¸: {len(labels_df)}\")\n",
    "    print(f\"  å°é½Šæ•¸: {len(aligned_df)}\")\n",
    "    print(f\"  åˆä½µæ•¸: {len(merged_df)}\")\n",
    "    print(f\"  æˆåŠŸåŒ¹é…: {merged_df['thermal_file'].notna().sum()}\")\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    return merged_df\n",
    "\n",
    "# åŸ·è¡Œåˆä½µ\n",
    "if ALIGNED_DIR.exists():\n",
    "    merged_df = merge_with_aligned_data(labels_df, ALIGNED_DIR)\n",
    "    \n",
    "    # å„²å­˜åˆä½µçµæœ\n",
    "    merged_csv = TRAINING_DIR / 'pose_labels_with_thermal.csv'\n",
    "    merged_df.to_csv(merged_csv, index=False, encoding='utf-8-sig')\n",
    "    print(f\"\\nğŸ’¾ åˆä½µè³‡æ–™å·²å„²å­˜: {merged_csv}\")\n",
    "    \n",
    "    print(f\"\\nå‰ 5 ç­†è³‡æ–™:\")\n",
    "    print(merged_df.head())\n",
    "else:\n",
    "    print(f\"\\nâš ï¸ å°é½Šè³‡æ–™ç›®éŒ„ä¸å­˜åœ¨: {ALIGNED_DIR}\")\n",
    "    merged_df = labels_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 7: ç”¢ç”Ÿè¨“ç·´æ‘˜è¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç”¢ç”Ÿè¨“ç·´æ‘˜è¦\n",
    "summary = {\n",
    "    'total_samples': len(merged_df),\n",
    "    'unique_pairs': merged_df['pair_id'].nunique(),\n",
    "    'pose_distribution': merged_df['pose_label'].value_counts().to_dict(),\n",
    "    'class_counts': {name: (merged_df['pose_class'] == idx).sum() \n",
    "                    for name, idx in CLASS_MAPPING.items()},\n",
    "    'paths': {\n",
    "        'labels_csv': str(output_csv),\n",
    "        'merged_csv': str(TRAINING_DIR / 'pose_labels_with_thermal.csv') if 'merged_df' in locals() else None,\n",
    "        'masks_dir': str(MASKS_DIR),\n",
    "        'aligned_dir': str(ALIGNED_DIR)\n",
    "    },\n",
    "    'statistics': {\n",
    "        'mean_mask_area': float(merged_df['mask_area'].mean()) if 'mask_area' in merged_df.columns else None,\n",
    "        'median_mask_area': float(merged_df['mask_area'].median()) if 'mask_area' in merged_df.columns else None\n",
    "    }\n",
    "}\n",
    "\n",
    "summary_json = TRAINING_DIR / 'training_summary.json'\n",
    "with open(summary_json, 'w', encoding='utf-8') as f:\n",
    "    json.dump(summary, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"è¨“ç·´è³‡æ–™æ‘˜è¦\")\n",
    "print(f\"{'='*70}\")\n",
    "print(json.dumps(summary, indent=2, ensure_ascii=False))\n",
    "print(f\"\\nğŸ’¾ æ‘˜è¦å·²å„²å­˜: {summary_json}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nâœ… æ‰€æœ‰è½‰æ›å®Œæˆï¼\")\n",
    "print(f\"\\nä¸‹ä¸€æ­¥: åŸ·è¡Œ 03_train_semantic_guided_sr.ipynb é–‹å§‹è¨“ç·´\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
